## Importance of the Control Plane

| Feature              | Role of Control Plane                                          |
|----------------------|----------------------------------------------------------------|
| Cluster Management   | Defines and applies the desired state (via YAMLs, Helm, etc.)  |
| Scheduling           | Decides where to run your containers                           |
| Scaling              | Adds/removes pods to match desired replicas                    |
| Monitoring & Healing | Restarts failed pods, reassigns workloads                      |
| Security             | Controls access through the API server                         |
| Networking           | Manages service discovery and routing                          |

## Key Components of the Control Plane

| kube-apiserver (API Server) | The Front Door of the Cluster     | All control plane communication goes through it (CLI, UI, kubelet, etc.).<br>Validates and processes REST requests (e.g., from kubectl).<br>Acts as a gatekeeper to the cluster.<br>Example: kubectl get podsÂ Â â†’Â Â hits the API server                                                                                                          |
| --------------------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| etcd                        | The Brain/Database of the Cluster | A distributed, key-value store used to store all cluster state.<br>Example: what pods are running, what nodes exist, configmaps, secrets, etc.<br>Highly criticalâ€”if etcd fails, the cluster loses its state.                                                                                                                                  |
| kube-scheduler              | The Job Allocator                 | Watches for new Pods without a node assigned.<br>Chooses the most suitable node based on:<br>Â â€¢ CPU/memory availability<br>Â â€¢ Node labels/taints<br>Â â€¢ Affinity rules                                                                                                                                                                          |
| kube-controller-manager     | The Automation Engine             | Runs several built-in controllers that continually reconcile desired state with actual state:<br>Â â€¢ Deployment Controller: Ensures the correct number of pods are running.<br>Â â€¢ Node Controller: Monitors node health.<br>Â â€¢ Job Controller, ReplicaSet Controller, etc.<br><br>Think of it like background services keeping things in check. |
| cloud-controller-manager    | Cloud Integration Layer           | Manages cloud-specific controller logic (e.g., for AWS, Azure, GCP).<br>Handles:<br>Â â€¢ Load balancer provisioning<br>Â â€¢ Node discovery<br>Â â€¢ Persistent volume integration with cloud storage                                                                                                                                                  |

### Analogy: Airport Control Tower
| Control Plane Component | Airport Equivalent                          |
|-------------------------|---------------------------------------------|
| kube-apiserver          | Air traffic controller                      |
| etcd                    | Flight schedule/records                     |
| scheduler               | Decides which runway (node) to use          |
| controller-manager      | Monitors flights, gates, baggage operations |

### Kubelet
The kubelet is the primary "node agent" that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider.

The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.

## The Core Principle

``` 
In EKS, the control plane does NOT initiate outbound connections into your VPC.
Worker nodes initiate and maintain connections to the control plane.
```

### Visual Flow

[ Pod crashes ]
      â†“
Node notices
      â†“  (outbound)
Control plane updates desired state
      â†“
Scheduler assigns pod
      â†“
Node sees assignment (watch)
      â†“
Node starts pod

## High-Level Architecture

Internet
   â†“
Route 53
   â†“
AWS ALB (via AWS Load Balancer Controller)
   â†“
Kubernetes Ingress
   â†“
Spring Cloud Gateway
   â†“
Microservices (person, address, ordering, etc.)
   â†“
Databases / External Services

## Control Plane Endpoint Access
EKS gives us 3 options:

| Option           | Meaning                      | Recommended  |
|------------------|------------------------------|--------------|
| Public only      | API accessible from internet | âŒ No         |
| Public + Private | Internet + VPC               | âš ï¸ Maybe     |
| Private only     | Only from VPC                | âœ… Yes (prod) |


## Recommended Subnet Layout
Public Subnet
  â””â”€â”€ ALB / NLB only

Private Subnet
  â”œâ”€â”€ EKS worker nodes
  â”œâ”€â”€ Pods
  â””â”€â”€ NAT Gateway route

Isolated Subnet
  â””â”€â”€ RDS / ElastiCache

##  Visual Mental Model
AWS Managed VPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EKS Control Plane        â”‚
â”‚ - API Server             â”‚
â”‚ - etcd                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Private endpoint
          â”‚
User VPC  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Private Subnets          â”‚
â”‚ - Worker Nodes           â”‚
â”‚ - Pods                   â”‚
â”‚                          â”‚
â”‚ Public Subnets           â”‚
â”‚ - ALB                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## Common Misconceptions

â€œControl plane should be in private subnetsâ€
â†’ You donâ€™t control that in EKS

â€œI need to open inbound ports to nodesâ€
â†’ No, nodes only need outbound

â€œAPI server must be public for kubectlâ€
â†’ No, use VPN / bastion

## Example: What Happens When we Deploy a Pod in EKS
	1.	Run kubectl apply -f deployment.yaml
	2.	kubectl sends request to EKS control plane (API server) endpoint.
	3.	The etcd stores the new desired state.
	4.	The scheduler assigns a node to the Pod.
	5.	The controller manager creates the necessary ReplicaSet/Pod resources.
	6.	The worker node (EC2/Fargate) pulls the container image and starts the Pod.

## Visual Mapping

+-------------------------------+
|         AWS Managed           |
|-------------------------------|
| kube-apiserver                |
| etcd                          |
| kube-scheduler                |
| kube-controller-manager       |
| cloud-controller-manager      |
+-------------------------------+
            |
            â†“
+-------------------------------+
|      VPC (User Managed)       |
|-------------------------------|
| Worker Nodes (EC2 or Fargate) |
| CoreDNS, kube-proxy, etc.     |
| Pods, Services, Volumes       |
+-------------------------------+

## Tools to Interact with EKS
	â€¢	eksctl â€” CLI tool to provision EKS clusters (recommended for setup).
	â€¢	kubectl â€” for managing resources (Pods, Services, etc.).
	â€¢	IAM Roles for Service Accounts (IRSA) â€” to give fine-grained AWS access to Pods.
	â€¢	CloudWatch â€” for logs and metrics (via container insights).

### Summary

| Task              | Recommendation                                         |
|-------------------|--------------------------------------------------------|
| Cluster           | Use eksctl or Terraform to create EKS                  |
| Ingress           | Use ALB Ingress Controller for HTTP(S) routing         |
| Deployment        | Use kubectl or Helm in CI/CD                           |
| YAMLs             | Structure per-service manifests, or use Helm charts    |
| Auth              | Use IAM Roles for Service Accounts                     |
| Gradual Migration | Migrate and test one service at a time                 |
| Monitoring        | Use CloudWatch, Prometheus, or Grafana for visibility  |
| Storage           | Use EFS or S3 where applicable                         |
| DNS               | Route 53 -> ALB Ingress -> Kubernetes Services         |


## K8 yaml config summary

| Element    | Field                     | Used For                                 |
|------------|---------------------------|------------------------------------------|
| Deployment | metadata.labels           | Identification (optional)                |
| Deployment | spec.selector.matchLabels | Links Deployment to Pods                 |
| Pod        | template.metadata.labels  | Must match selector above                |
| Service    | spec.selector             | Routes traffic to Pods                   |
| Container  | name                      | Internal name only (not used in linking) |

### Visual map
[Deployment]
  â””â”€â”€ spec.selector.matchLabels = app: service-a
         â””â”€â”€ matches â†’
             [Pods created by Deployment]
                 â””â”€â”€ metadata.labels = app: service-a
                       â””â”€â”€ matched by â†’
                           [Service]
                             â””â”€â”€ spec.selector = app: service-a

## Deployment vs Service

| Resource   | Purpose                                | Key Concepts                       |
|------------|----------------------------------------|------------------------------------|
| Deployment | Declaratively manage Pods and scaling  | Replicas, selectors, templates     |
| Service    | Network abstraction for accessing pods | DNS, load balancing, service types |

## Notes

```
Kubernetes is eventually consistent, not continuously dependent.

etcd is strong consistency.

Everything in Kubernetes is state â€” and state lives in etcd.
```

- Make sure to set ```discover-enabled``` label to make services discovered by spring k8 discovery client.

## Autoscaling
- HPA (Horizontal Pod Autoscaler) - Sclaes Pods
- CA (Cluster Autoscaler) - Scales Nodes

Traffic â†‘
   â†“
HPA scales pods
   â†“
No node capacity
   â†“
Cluster Autoscaler adds nodes


### Rolling Updates (Zero downtime)
Deployment Update Flow
	1.	New ReplicaSet created
	2.	New pods started
	3.	Readiness probes pass
	4.	Old pods terminated gradually

### What happens when a Node dies

Node dies
   â†“
Kubelet stops heartbeats
   â†“
Control plane marks NotReady
   â†“
Pods evicted
   â†“
Scheduler reschedules
   â†“
ASG (Auto Scaling Group) launches new node

### PodDisruptionBudget (PDB)
- A PodDisruptionBudget defines how many pods of a workload are allowed to be unavailable during ```voluntary disruptions```.
- PDB tells Kubernetes: â€œDonâ€™t take down too much of my app at once.â€
- A PDB defines one of two constraints:

```
minAvailable: 2
or 
maxUnavailable: 1

```

#### Voluntary Disruption

Things Kubernetes (or operators) choose to do:
 - Node draining (for upgrades)
 - Cluster scaling down
 - Rolling updates
 - Manual kubectl drain
 - Managed node group updates

#### Example PDB
```
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: person-service-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: person-service
```
This means:
	Kubernetes will never voluntarily evict more than 1 pod at a time



#### Mental Model
- Pods heal your app.
- Nodes heal your infrastructure.
- Kubernetes orchestrates both.

## Configmap vs Secrets
- Both are stored in etcd
| Layer              | ConfigMaps | Secrets          |
|--------------------|------------|------------------|
| etcd storage       | Plaintext  | Base64           |
| At-rest encryption | âŒ          | âœ… (EKS default)  |
| In-transit (TLS)   | âœ…          | âœ…                |

### Best Practice in EKS

| Use case       | Recommended          |
|----------------|----------------------|
| App config     | ConfigMap            |
| DB credentials | AWS Secrets Manager  |
| AWS access     | IRSA                 |
| TLS certs      | cert-manager + ACM   |


## RBAC
### Two layers in EKS

AWS IAM (Who can talk to the cluster)
        â†“
Kubernetes RBAC (What they can do inside)

### Kubernetes RBAC

``` â€œOnce inside the cluster, what are you allowed to do?â€ ```

### Subjects
- Users
- Groups
- Service Accounts

### RABC Objects

| Object             | Purpose                       |
|--------------------|-------------------------------|
| Role               | Permissions in one namespace  |
| ClusterRole        | Permissions cluster-wide      |
| RoleBinding        | Attach Role to subject        |
| ClusterRoleBinding | Attach ClusterRole to subject |

### Example role and rolebinding

```
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

```
```
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: ServiceAccount
  name: app-sa
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
### How EKS Authenticates Users
kubectl
  â†“ (IAM auth)
AWS STS
  â†“
EKS API Server
  â†“
Kubernetes RBAC

### aws-auth ConfigMap

- aws-auth is the bridge between AWS IAM and Kubernetes RBAC.
- Lives in kube-system namespace
- Maps IAM roles &  IAM users â†’ to Kubernetes users/groups
It defines:
	â€¢	Who can authenticate to Kubernetes
	â€¢	What Kubernetes identity they assume

### aws-auth ConfigMap workflow

Human
 â†“
Assume IAM role (e.g. eks-dev-role)
 â†“
Allowed: eks:DescribeCluster
 â†“
EKS API authenticates via aws-auth
 â†“
Mapped to K8s group (e.g. developers)
 â†“
RBAC allows actions

### RABC - IAM Role Association

Kubernetes Role / ClusterRole
        â†“
RoleBinding / ClusterRoleBinding
        â†“
Kubernetes Group   (string)
        â†‘
aws-auth ConfigMap
        â†‘
IAM Role

### Example

```
# Kubernetes Role (namespace-scoped permissions)

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev-read-role
  namespace: dev
rules:
  - apiGroups: [""]
    resources:
      - pods
      - services
      - configmaps
    verbs:
      - get
      - list
      - watch

  - apiGroups: ["apps"]
    resources:
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch

# RoleBinding (bind role to a GROUP)

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-read-binding
  namespace: dev
subjects:
  - kind: Group
    name: developers   # <-- Kubernetes group name
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: dev-read-role
  apiGroup: rbac.authorization.k8s.io


# aws-auth ConfigMap (IAM â†’ K8s mapping)

apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system  # built-in namespace
data:
  mapRoles: |
    - rolearn: arn:aws:iam::123456789012:role/eks-dev-role
      username: dev-user
      groups:
        - developers

```

### RABC apiGroups attribute reference table

| Resource    | apiGroup                  |
|-------------|---------------------------|
| pods        | ""                        |
| services    | ""                        |
| configmaps  | ""                        |
| secrets     | ""                        |
| deployments | apps                      |
| replicasets | apps                      |
| ingresses   | networking.k8s.io         |
| jobs        | batch                     |
| roles       | rbac.authorization.k8s.io |

### Who gets what

| Persona       | Access                   |
|---------------|--------------------------|
| Platform team | ClusterRole              |
| App teams     | Namespace Roles          |
| CI/CD         | Limited ClusterRole      |
| Humans        | Read-only where possible |


### Notes
- Pods authenticate using ServiceAccounts
- IAM decides â€œwho you areâ€
- RBAC decides â€œwhat you can doâ€
- We dot not create RABC Groups. Group exists only by name.
- Make sure to include the correct ```apiGroup``` in the role definition.
- ```kube-system``` is a built-in namespace created automatically when a cluster is initialized.
- Make sure to add toleration for coreDNS addon
- Make sure to add ```before_compute``` flag for vpc-cni and eks-pod-identity-agent addons. Without that the nodes can't register with API server.

### How to access the API server using kubectl

User logs in via SSO
   â†“
Assumes IAM role (e.g., EKS-Dev-Admin)
   â†“
kubectl signs request with temporary credentials
   â†“
EKS API sees principal ARN
   â†“
EKS API checks access entry
   â†“
If authorized â†’ Kubernetes RBAC enforces permissions

1) Create an IAM policy with necessary access to eks cluster (e.g. eks:DescribeCluster, eks:DescribeNodegroup, etc)
2) Create an IAM role (e.g trocks-eks-iam-role) and add the policy created in step 1.
3) Create an IAM user and add permission (sts:assumeRole) to assume the role created in step 2.
4) Map the IAM role created in step 2 to a cluster policy (e.g. AmazonEKSClusterAdminPolicy) in the EKS console.
As an IAM user (created in step 3),
    1) Run ``` aws eks update-kubeconfig --name trocks-manual --region us-east-2 --role-arn <role:arn created in step 2>``` to update the kube config (~/.kube/config). Configures kubectl so that you can connect to an Amazon EKS cluster.
    e.g. 
    ```
    aws eks update-kubeconfig --name trocks-manual --region us-east-2 --role-arn arn:aws:iam::XXXXXXXXX:role/trocks-eks-iam-role
    ```

    2) Now, use kubectl to access the cluster.
    e.g.
    
    ``` kubectl get pods  ```

### How to give terraform access to EKS
- Resources are created in phases.
Phase 1 â†’ EKS control plane exists (Create Infra resources like IAM, VPC, Cluster (with access_entry))
    - access_entry - maps IAM role to a cluster role.
Phase 2 â†’ IAM â†” EKS access entries exist
Phase 3 â†’ Kubernetes provider authenticates (configure kubernetes provider configuration to authenticate with EKS)
    - This phase will use the cluster_ca_certificate and aws eks get-token to authenticate with EKS.
Phase 4 â†’ Kubernetes resources are created (e.g namespaces, configmaps, etc)

## EKS deployment

Internet
   â†“
Route 53
   â†“
ALB (AWS Load Balancer Controller)
   â†“
Ingress (K8s)
   â†“
Spring Cloud Gateway
   â†“
Person Service  â†”  Address Service
          â†‘
     Spring Cloud Config



                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚          Clients             â”‚
                      â”‚  (Browser, Mobile, CI/CD)    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Route 53 DNS  â”‚
                          â”‚  (api.trocks.com) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                          Public / Private ALB / NLB
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EKS Cluster Control Plane            â”‚
â”‚  (AWS-managed: API Server, Scheduler, etcd, Controllers)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–²                      â–²
                   â”‚                      â”‚
                   â”‚                      â”‚
                   â”‚                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                               â”‚
        â–¼                                               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  System Node  â”‚                           â”‚   App Node     â”‚
 â”‚   Group       â”‚                           â”‚   Group(s)     â”‚
 â”‚ (tainted)     â”‚                           â”‚ (untainted)    â”‚
 â”‚---------------â”‚                           â”‚----------------â”‚
 â”‚ CoreDNS       â”‚                           â”‚ person-service â”‚
 â”‚ kube-proxy    â”‚                           â”‚ address-serviceâ”‚
 â”‚ VPC CNI       â”‚                           â”‚ cloud-config   â”‚
 â”‚ CloudWatch    â”‚                           â”‚ cloud-gateway  â”‚
 â”‚ ALB Controllerâ”‚                           â”‚ other services â”‚
 â”‚ MetricsServer â”‚                           â”‚ HA / replicas  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                                               â–²
        â”‚                                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         VPC Private Subnets
                         (ENIs for control plane)

### Tools
- ```eksctl``` is a command-line tool that simplifies creating, managing, and operating Amazon Elastic Kubernetes Service (EKS) clusters, automating complex tasks like VPC setup and IAM role creation through simple CLI commands or declarative YAML files, making EKS management easier for DevOps engineers and platform teams.

- ```kubectl```: The kubectl command line tool is the main tool you will use to manage resources within your Kubernetes cluster. This page describes how to download and set up the kubectl binary that matches the version of your Kubernetes cluster.

-- ```helm```: package manager for Kubernetes

### Recommended Approach to Create & Manage EKS
| Tool            | Use it for     | Recommended  |
|-----------------|----------------|--------------|
| AWS Console     | Viewing only   | âœ… Yes        |
| eksctl          | PoC / learning | âš ï¸ Limited   |
| Terraform       | Prod infra     | âœ… YES        |
| Helm            | Add-ons        | âœ… YES        |
| kubectl         | Debugging      | âœ… YES        |
| GitOps (ArgoCD) | App deploy     | â­ Best       |

### EKS Modes

| Mode                      | Who manages nodes | Who manages scaling | Infra control | Typical use             |
|---------------------------|-------------------|---------------------|---------------|-------------------------|
| Managed Node Groups (EC2) | You               | You                 | High          | Most prod workloads     |
| Self-managed Nodes        | You               | You                 | Full          | Custom OS / edge cases  |
| EKS Fargate               | AWS               | AWS                 | Low           | Small, bursty, secure   |
| EKS Auto Mode             | AWS               | AWS                 | Very low      | New / simplified ops    |

### Two Ways to Provision EKS with Terraform
- Using the terraform-aws-eks module (Community Module) - terraform-aws-modules/eks/aws
- Using Raw aws_eks_ Resources*

### When Are kube-system Pods Created
Step 1: EKS cluster created
	â€¢	Control plane comes up (AWS-managed)
	â€¢	No nodes yet
	â€¢	No pods yet

Step 2: Node group created
	â€¢	EC2 instances boot
	â€¢	kubelet registers nodes
	â€¢	Nodes appear in cluster

Step 3: Core system pods are created
Immediately after nodes appear:
	â€¢	CoreDNS
	â€¢	kube-proxy
	â€¢	VPC CNI
	â€¢	Other add-ons

These pods are created by:
	â€¢	Kubernetes controllers
	â€¢	EKS add-on manager

ğŸ‘‰ They are created ONLY AFTER nodes exist

### Taints
- A taint is a â€œKEEP OUTâ€ sign placed on a node.
- It tells Kubernetes: â€œDo not schedule pods here unless the pod explicitly allows (tolerates) it.â€
- Syntax: ``` key=value:effect ```
- Example, ```node-role=system:NoSchedule```

#### The 3 Taint Effects
- NoSchedule - New pods will not be scheduled unless tolerated
- PreferNoSchedule - Scheduler tries to avoid, but may schedule
- NoExecute - Pod is evicted immediately if it doesnâ€™t tolerate

#### Toleration
- A toleration is a pod saying: â€œI am allowed past this taint.â€
- Without toleration â†’ pod is rejected.

#### Toleration example with Taints
```
- key: "node-role"
  operator: "Equal"
  value: "system"
  effect: "NoSchedule"
```
This pod can run on nodes tainted with:
``` node-role=system:NoSchedule ```

- ``` node-role.kubernetes.io/system=true:NoSchedule``` - This means that Keep all pods out of this node unless the pod explicitly tolerates this taint. Since system pods tolerate (AWS sets toleration for system pods) this taint, they are allowed to run in the tainted nodes whereas the app pods do not have the toleration set so they are KEPT OUT of these tainted nodes. 

- The toleration does NOT mean â€œsystem pods must run here.â€ It only means: â€œSystem pods are allowed to run hereâ€

### Cluster Auth Mode vs API Server access
- Cluster authentication mode controls Who can authenticate (IAM â†’ Kubernetes)
- API server endpoint access controles From where the API server can be reached

### Node group best practices
- kube-system Pods can go to any node group that the scheduler chooses. Use 'label' and 'taint' to place the kube-system pods in a dedicated node group.
- Do not mix kube-system pods and application pods in the same node group.
- EKS will never magically isolate workloads. We must design the isolation using 'label' and 'taint'.
- Use 'untainted' node groups for app node groups.

## Best Practices
- Never run prod services with 1 replica.
- PodDisruptionBudget - Prevents too many pods from being evicted at once.

## Helm
- Kubernetes/Helm, however, did create a "Secret" to track the release before it crashed.
- Helm cannot detect drifts.

## Deployement approach
###
Layer,Tool,Your Logic
Infra,Terraform,"VPC, EKS, IAM, and the ALB Controller."
CI,CodePipeline,Build Docker image â†’ Push to ECR.
CD,CodePipeline,Runs a script to helm upgrade your app.


Infrastructure (Terraform): Still used for VPC, EKS, and "Cluster Bootstrapping" (installing the ALB Controller and ArgoCD).

CI (CodePipeline): Still builds the Docker image, but instead of "deploying," it simply updates a Git repo with the new image tag.

CD (ArgoCD): An agent inside the cluster watches your Git repo. When it sees the new tag, it automatically "pulls" the change and updates the cluster.

## ArgoCD

ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes.

```Think of it as a "Permanent Watchman" that sits between your Git repository (the "Truth") and your Kubernetes cluster (the "Reality"). Its sole job is to ensure that what is defined in Git is exactly what is running in your cluster.```

### Capabilities
1) ArgoCD constantly compares the Desired State (Git) with the Live State (EKS).
  - It checks the cluster every 3 minutes (by default).
2) Automatic Drift Detection & Correction.
3) Multi-Cluster Management.
4) Support for Multiple Manifest Tools

### How ArgoCD "Uses" Helm Internally

When you point ArgoCD at a Helm chart, it goes through a three-step process:

1) Rendering (helm template): ArgoCD runs the equivalent of helm template . --values values.yaml. It takes your chart and your values and turns them into raw, plain Kubernetes YAML manifests.

2) Comparison (The "Diff"): It compares that raw YAML against what is currently running in your EKS cluster.

3) Application (kubectl apply): If thereâ€™s a difference, it sends that raw YAML to the Kubernetes API using the same logic as kubectl apply.

## References
- https://docs.aws.amazon.com/cli/latest/reference/eks/update-kubeconfig.html
- https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/eks-managed-node-group/eks-bottlerocket.tf
- https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/6.3.0
- https://github.com/aws/eks-charts/tree/master/stable/aws-load-balancer-controller
- https://artifacthub.io/packages/helm/aws/aws-load-balancer-controller
- https://helm.sh/docs/topics/charts/
- https://helm.sh/docs/chart_template_guide/values_files/
- https://developer.hashicorp.com/terraform/tutorials/kubernetes/helm-provider?in=terraform%2Fkubernetes


